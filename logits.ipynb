{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "print = lambda *args, **kwargs: __builtins__.print(\n",
    "    *[arg.replace('Ä ', ' ') if isinstance(arg, str) else arg for arg in args],\n",
    "    **kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6f0a94d178492d9dff78d3de666bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 128000\n",
      "Embedding matrix shape: torch.Size([128256, 3072])\n",
      "Output embedding matrix shape: torch.Size([128256, 3072])\n"
     ]
    }
   ],
   "source": [
    "# Model information\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(f\"Embedding matrix shape: {model.get_input_embeddings().weight.shape}\")\n",
    "\n",
    "output_embeddings = model.get_output_embeddings().weight\n",
    "print(f\"Output embedding matrix shape: {output_embeddings.shape}\")  # [vocab_size, hidden_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query shape: torch.Size([1, 5])\n",
      "Tokenized query: ['<|begin_of_text|>', 'The', ' quick', ' brown', ' fox']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "query = \"The quick brown fox\"\n",
    "tokenized_query = tokenizer(query, return_tensors=\"pt\")\n",
    "input_ids = tokenized_query['input_ids']\n",
    "print(f\"Tokenized query shape: {input_ids.shape}\")\n",
    "print(f\"Tokenized query: {[tokenizer.decode(token_id) for token_id in input_ids[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing logits\n",
    "query = \"The quick brown fox\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **tokenizer(query, return_tensors=\"pt\")\n",
    "    )\n",
    "    logits = outputs.logits\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_sampler(logits):\n",
    "    return torch.argmax(logits, dim=-1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(input_ids, max_length=50, sampler=greedy_sampler):\n",
    "    for _ in range(max_length):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            next_token = sampler(logits)\n",
    "            input_ids = torch.cat([input_ids, torch.tensor([[next_token]])], dim=-1)\n",
    "            if next_token == model.generation_config.eos_token_id:\n",
    "                break\n",
    "    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "This is a well-known pangram, a sentence that uses all the letters of the alphabet at least once. It is often used as a demonstration of a font or keyboard's capabilities, as it is a concise way to\n"
     ]
    }
   ],
   "source": [
    "query = \"The quick brown fox\"\n",
    "input_ids = tokenizer(query, return_tensors=\"pt\")['input_ids']\n",
    "max_length = 50\n",
    "\n",
    "print(generator(input_ids, max_length=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(input, dim, temperature=1.0):\n",
    "    return torch.softmax(input/temperature, dim=dim)\n",
    "\n",
    "def top_k(input, k=50):\n",
    "    if k > 0:\n",
    "        top_k_values, top_k_indices = torch.topk(inputs, min(top_k, inputs.shape[-1]))\n",
    "        k_mask = torch.zeros_like(inputs, dtype=torch.bool)\n",
    "        k_mask.scatter_(-1, top_k_indices, True)\n",
    "        inputs = torch.where(k_mask, inputs, torch.tensor(float('-inf')))\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def top_p(input, p=0.9):\n",
    "    sorted_probs, sorted_indices = torch.sort(input, descending=True)\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "    \n",
    "    sorted_mask = cumulative_probs <= p\n",
    "    \n",
    "    sorted_mask[..., 0] = True\n",
    "    mask = torch.zeros_like(input, dtype=torch.bool).scatter_(-1, sorted_indices, sorted_mask)\n",
    "    masked_logits = torch.where(mask, input, torch.tensor(float('-inf')))\n",
    "    \n",
    "    return masked_logits\n",
    "\n",
    "def multinomial_sampler(temp=1.0, top_k=0, top_p=1.0):\n",
    "    def sampler(logits):\n",
    "        if top_k > 0:\n",
    "            logits = top_k(logits, k=top_k)\n",
    "        if top_p < 1.0:\n",
    "            logits = top_p(logits, p=top_p)\n",
    "        if temp != 1.0:\n",
    "            logits = logits / temp\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        samples = torch.multinomial(probs, num_samples=1)\n",
    "        return samples\n",
    "    return sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n",
      "This is a well-known pangram, a sentence that uses all the letters of the alphabet at least once. It is often used as a demonstration of a font or keyboard's capabilities.\n",
      "\n",
      "I have seen this sentence in various\n"
     ]
    }
   ],
   "source": [
    "query = \"The quick brown fox\"\n",
    "input_ids = tokenizer(query, return_tensors=\"pt\")['input_ids']\n",
    "max_length = 50\n",
    "\n",
    "print(generator(input_ids, max_length=max_length, sampler=multinomial_sampler(temp=0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_sampler(input_ids, model, beam_width=2, max_length=50):\n",
    "    beams = [(input_ids, 0.0)]\n",
    "    for step in range(max_length):\n",
    "        candidates = []\n",
    "        for sequence, score in beams:\n",
    "            if sequence[0, -1].item() == model.generation_config.eos_token_id:\n",
    "                candidates.append((sequence, score))\n",
    "                continue\n",
    "            with torch.no_grad():\n",
    "                outputs = model(sequence)\n",
    "                logits = outputs.logits[:, -1, :]\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)\n",
    "                top_log_probs, top_indices = torch.topk(log_probs[0], beam_width)\n",
    "                for i in range(beam_width):\n",
    "                    token_id = top_indices[i].item()\n",
    "                    token_log_prob = top_log_probs[i].item()\n",
    "                    new_sequence = torch.cat([sequence, torch.tensor([[token_id]])], dim=1)\n",
    "                    new_score = score + token_log_prob\n",
    "                    candidates.append((new_sequence, new_score))\n",
    "        candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "        beams = candidates[:beam_width]\n",
    "        if all(beam[0][0, -1].item() == model.generation_config.eos_token_id for beam in beams):\n",
    "            break\n",
    "    return beams\n",
    "\n",
    "def beam_generator(input_ids, beam_width=2, max_length=50):\n",
    "    beams = beam_search_sampler(input_ids, model, beam_width, max_length)\n",
    "    best_sequence = beams[0][0]\n",
    "    return tokenizer.decode(best_sequence[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog\n",
      "The quick brown fox jumps over the lazy dog\n",
      "The quick brown fox jumps over the lazy dog\n",
      "The quick brown fox jumps over the lazy dog\n",
      "The quick brown fox jumps over the lazy dog\n",
      "The quick brown fox\n"
     ]
    }
   ],
   "source": [
    "query = \"The quick brown fox\"\n",
    "input_ids = tokenizer(query, return_tensors=\"pt\")['input_ids']\n",
    "max_length = 50\n",
    "print(beam_generator(input_ids, beam_width=3, max_length=max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI JSON Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Alice and Bob are going to a science fair on Friday.\"},\n",
    "    ],\n",
    "    response_format=CalendarEvent,\n",
    ")\n",
    "\n",
    "event = completion.choices[0].message.parsed\n",
    "event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerRequest(department='technical', intent='login issue', priority='high')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CustomerRequest(BaseModel):\n",
    "    department: Literal[\"billing\", \"technical\", \"account\", \"product\"]\n",
    "    intent: str\n",
    "    priority: Literal['high', 'medium', 'low']\n",
    "\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Classify the user message.\"},\n",
    "        {\"role\": \"user\", \"content\": \"I can't login to my computer.\"},\n",
    "    ],\n",
    "    response_format=CustomerRequest,\n",
    ")\n",
    "\n",
    "request = completion.choices[0].message.parsed\n",
    "request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c499afe0ad46b4abc240b620094a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "from outlines import generate, samplers, models\n",
    "\n",
    "outlines_model = models.transformers(\"meta-llama/Llama-3.2-3B-Instruct\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emek/Desktop/foss/pycon/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/Users/emek/Desktop/foss/pycon/.venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PyCon is the largest gathering of Python programmers in the world. It is a conference that brings together\n"
     ]
    }
   ],
   "source": [
    "# Samplers\n",
    "m_sampler = samplers.multinomial(\n",
    "    3,  # number of samples\n",
    "    temperature=0.5,  # temperature for sampling\n",
    "    top_k=5,  # top-k sampling\n",
    "    top_p=0.9,  # top-p sampling\n",
    ")\n",
    "\n",
    "g_sampler = samplers.greedy()\n",
    "\n",
    "b_sampler = samplers.beam_search(\n",
    "    beams=5  # sequences to keep\n",
    ")\n",
    "\n",
    "generator = generate.text(outlines_model, g_sampler)\n",
    "answer = generator(\"What is PyCon?\", max_tokens=20)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'billing'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dept_classifier = generate.choice(\n",
    "    outlines_model, \n",
    "    [\"billing\", \"technical\", \"account\", \"product\"]\n",
    ")\n",
    "answer = dept_classifier(\"How do I reset my password for?\")\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=12345 name='Sony X950B' price=299.99 in_stock=True\n"
     ]
    }
   ],
   "source": [
    "class Product(BaseModel):\n",
    "   id: int\n",
    "   name: str\n",
    "   price: float\n",
    "   in_stock: bool\n",
    "\n",
    "product_generator = generate.json(outlines_model, Product)\n",
    "product = product_generator(\"Create a product entry for headphones\")\n",
    "print(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+49-143-494-6969\n"
     ]
    }
   ],
   "source": [
    "phone_regex_validator = generate.regex(\n",
    "    outlines_model,\n",
    "    r\"\\+\\d{2}-\\d{3}-\\d{3}-\\d{4}\"\n",
    ")  # only allow +49-123-456-7890 format\n",
    "\n",
    "prompt = \"Properly format the phone number in this text: \"\n",
    "user_input = \"Number: 123 456 1234, Country: DE\"\n",
    "valid_format = phone_regex_validator(prompt + user_input)\n",
    "print(valid_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emek/Desktop/foss/pycon/.venv/lib/python3.11/site-packages/outlines/fsm/guide.py:110: UserWarning: Outlines' public *community-contributed* CFG structured generation is experimental. Please review https://dottxt-ai.github.io/outlines/latest/reference/generation/cfg#disclaimer\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4 - 2.  ---  4 - 2.  --- \n"
     ]
    }
   ],
   "source": [
    "from outlines import grammars\n",
    "\n",
    "arithmetic_grammar = grammars.arithmetic\n",
    "\n",
    "generator = generate.cfg(outlines_model, arithmetic_grammar)\n",
    "sequence = generator(\n",
    "  \"Alice had 4 apples and Bob ate 2. \"\n",
    "  + \"Write an expression for Alice's apples:\",\n",
    "  max_tokens=20,\n",
    ")\n",
    "\n",
    "print(sequence)\n",
    "# (8-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f929bf944e0f469a895a0f657ea8f436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emek/Desktop/foss/pycon/.venv/lib/python3.11/site-packages/guidance/models/_transformers.py:162: UserWarning: Could not build_byte tokens from the tokenizer by encoding token strings: Round-trip encoding of tokens [!] failed! Got [128000, 0]\n",
      "  warnings.warn(\n",
      "/Users/emek/Desktop/foss/pycon/.venv/lib/python3.11/site-packages/guidance/chat.py:80: UserWarning: Chat template {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      " was unable to be loaded directly into guidance.\n",
      "                        Defaulting to the ChatML format which may not be optimal for the selected model. \n",
      "                        For best results, create and pass in a `guidance.ChatTemplate` subclass for your model.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb930e54f4c491f9e3af1a5996dd10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance import models, gen, select\n",
    "\n",
    "guidance_model = models.Transformers(\"meta-llama/Llama-3.2-3B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f448889cf1e4e358149308ffbc1404a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's your favorite season? I prefer summer.\n",
      "Let me explain why: Summer is the best season for me because it's warm and sunny, and I love spending time outdoors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = guidance_model + f'''\\\n",
    "What's your favorite season? I prefer {select(['summer', 'winter'])}.\n",
    "Let me explain why: {gen(stop=\".\")}.\n",
    "'''\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5875ee3eb324e31b49630862cff4fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "StitchWidget(initial_height='auto', initial_width='100%', srcdoc='<!doctype html>\\n<html lang=\"en\">\\n<head>\\n â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"Asia is a vast and diverse continent, offering countless travel options. Here are some suggestions based on different interests:\\n\\n**Cultural and Historical**\\n\\n1. **Japan**: Explore Tokyo's neon streets, visit ancient temples and shrines, and experience the unique culture of Kyoto.\\n2. **Thailand**: Discover the bustling streets of Bangkok, relax on the beautiful beaches of Phuket, and visit the ancient city of Chiang Mai.\\n3. **India**: Visit the Taj Mahal, explore the\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from guidance import system, user, assistant\n",
    "\n",
    "with system():\n",
    "    lm = guidance_model + \"You are a travel advisor.\"\n",
    "\n",
    "with user():\n",
    "    lm += \"I want to travel to Asia. Any suggestions?\"\n",
    "\n",
    "with assistant():\n",
    "    lm += gen(\"response\", max_tokens=100)\n",
    "\n",
    "lm[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
